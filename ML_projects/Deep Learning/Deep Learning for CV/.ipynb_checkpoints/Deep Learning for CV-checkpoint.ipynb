{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* Learning about using neural networks (NN) and deep learning (DL) methods in general for computer vision (CV) problems\n",
    "* Load and prepare iamge data, such as photographs, for modeling using Python libraries\n",
    "* How specialized layers for image data work, including 1D and 2D convolutions, max and average pooling, and intuitions for the impact that each layer has on input data\n",
    "* How to configure convolutional layers, including aspects such as filter size, stride, and pooling\n",
    "* How key modeling innovations for CNN work and how to implement them from scratch, such as VGG blocks, inception models, and resnet modules\n",
    "* How to develop, tune, evaluate and make predictions with CNN on standard benchmark CV datasets for image classification, such as Fashion-MNIST and CIFAR-10\n",
    "* How to develop, tune, evaluate, and make predictions with CNN on entirely new datasets for image classification, such as satellite photographs and photographs of pets\n",
    "* How to use techniques such as pre-trained models, transfer learning and image augmentation to accelerate and improve model development\n",
    "* How to use pre-trained models and develop new models for object recognition tasks, such as object localization and object detection in photographs, using techniques like R-CNN and YOLO\n",
    "* How to use DL models for face recognition tasks, such as face identification and face verification in photographs, using techniques like Google's FaceNet and Oxford's VGGFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure:**\n",
    "\n",
    "1. Part 1: Foundations.\n",
    "    * Introduction to CV and DL\n",
    "    * Getting started with Keras\n",
    "2. Part 2: Data Preparation:\n",
    "    * How to load images, image datasets\n",
    "    * Techniques for scaling pixel data in order to make images ready for modeling\n",
    "3. Part 3: Convolutions and Pooling:\n",
    "    * How building blocks of CNN work\n",
    "    * Convolutions and Pooling layers\n",
    "4. Part 4: CNN\n",
    "    * CNN model architectural innovations and how to implement\n",
    "5. Part 5: Image Classification\n",
    "    * Develop, tune, evaluate deep CNN for image classification\n",
    "    * Problems like Fashion-MNIST, CIFAR-10, and new datasets\n",
    "6. Part 6: Object Detection\n",
    "    * DL models for object detection such as R-CNN and YOLO\n",
    "    * How to use pre-trained models\n",
    "    * Train models for new object detection datasets\n",
    "7. Part 7: Face Recognition\n",
    "    * DL models for face recognition, including FaceNet and VGGFace\n",
    "    * How to use pre-trained models for face identification and face verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV vs Image Processing**\n",
    "* CV is to extract useful meaning out of the digital input\n",
    "* A given CV system may require image proccessing to be applied to raw input (eg. pre-processing images)\n",
    "* Examples of image processing include:\n",
    "    - Normalizing photometric properties of the image such as brightness or color\n",
    "    - Cropping bounds of image, such as centering an object in photograph\n",
    "    - Removing digital noise from image, such as digital artifacts from low light levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Popular CV application usage for objects:**\n",
    "* Classification: What broad category of object is in this photo?\n",
    "* Identification: Which type of a given object is in this photo?\n",
    "* Verification: Is the object in the photo?\n",
    "* Detection: Where are the objects in the photo?\n",
    "* Landmark Detection: What are the key points for the object in the photo?\n",
    "* Segmentation: What pixels belong to the object in the image?\n",
    "* Recognition: What objects are in this photo, and where are they?\n",
    "* Others: Information retrieval - Finding new images like an existing image or images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Promises of DL**\n",
    "* Automatic Feature Extraction: Features can be automatically learned and extracted from raw image data\n",
    "    - Deep NNs are getting better at image classification, single-object localization, and object detection\n",
    "* End-to-End models: Single end-to-end models can replace pipelines of specialized models\n",
    "* Model Reuse: Learned features and even entire models can be reused across tasks\n",
    "    - Transfer learning, pre-trained models can be used to extract useful general features from digital images and can also be fine-tuned, tailored to the specifics of the new task, saving lots of time and resources\n",
    "* Superior Performance: Techniques demonstrate better skill on challenging tasks\n",
    "* General Method: Single general method can be used on range of related tasks\n",
    "    - Single general class of model (CNNs) can be configured and used across each CV task directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of DL network models**\n",
    "* CNN - Specifically designed for image data\n",
    "* MLP - Useful for developing models that make predictions given the learned features extracted by CNNs\n",
    "* RNNs - eg. LSTMs, helpful when working with sequences of images over time, such as with video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DL have seen the most success in recent times in the areas of:**\n",
    "* Mainly Object Recognition categories:\n",
    "    - Optical Character Recognition (OCR)\n",
    "    - Image Classification\n",
    "    - Object Detection\n",
    "    - Face Detection\n",
    "    - Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to develop DL models with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Model Life-Cycle:**\n",
    "1. Define network\n",
    "    - Add layers\n",
    "    - Input layer (need to specify input_dim = xx)\n",
    "    - Hidden layers\n",
    "    - Output layer (Regression - No activation function, Binary Class - Sigmoid, Multiclass - Softmax)\n",
    "2. Compile network\n",
    "    - specify optimizer algorithm (SGD - tune lr and momentum, Adam - tune lr, RMSprop - tune lr)\n",
    "    - specify loss function (Regression - mean_squared_error, Binary Class - binary_crossentropy, Multiclass - categorical_crossentropy)\n",
    "    - specify metrics (generally is accuracy for classification problems)\n",
    "3. Fit network\n",
    "    - specify batch_size\n",
    "    - specify epochs\n",
    "    - History object is returned after model is fit, that contains summary of performance of model during training\n",
    "4. Evaluate network\n",
    "5. Make predictions\n",
    "    - model.predict(X)\n",
    "    - model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Network Models built using Keras functional API**\n",
    "* MLP\n",
    "* CNN\n",
    "* RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP**\n",
    "* Example model below has:\n",
    "    - 10 inputs\n",
    "    - 3 hidden layers with 10, 20, and 10 neurons\n",
    "    - output layer with 1 output\n",
    "    - RELU used for each hidden layer\n",
    "    - Sigmoid activation function used in output layer, for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a multilayer perceptron\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "visible = Input(shape=(10,))\n",
    "hidden1 = Dense(10, activation='relu')(visible)\n",
    "hidden2 = Dense(20, activation='relu')(hidden1)\n",
    "hidden3 = Dense(10, activation='relu')(hidden2)\n",
    "output = Dense(1, activation='sigmoid')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "model.summary()\n",
    "# plot graph\n",
    "plot_model(model, to_file='multilayer_perceptron_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**\n",
    "* Example model below has:\n",
    "    - Model receives 64x64 images as input\n",
    "    - Sequence of 2 convolutional and pooling layers as features extractors\n",
    "    - Followed by a fully connected layer to interpret the features\n",
    "    - Finally an output layer with sigmoid activation for 2 class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 61, 61, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 16)        8208      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                27050     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 35,813\n",
      "Trainable params: 35,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a convolutional neural network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "visible = Input(shape=(64,64,1))\n",
    "conv1 = Conv2D(32, (4,4), activation='relu')(visible)\n",
    "pool1 = MaxPooling2D()(conv1)\n",
    "conv2 = Conv2D(16, (4,4), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D()(conv2)\n",
    "flat1 = Flatten()(pool2)\n",
    "hidden1 = Dense(10, activation='relu')(flat1)\n",
    "output = Dense(1, activation='sigmoid')(hidden1)\n",
    "\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()\n",
    "\n",
    "# plot graph\n",
    "plot_model(model, to_file='convolutional_neural_network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**\n",
    "* Example LSTM RNN model below has:\n",
    "    - 100 time steps of 1 feature as input\n",
    "    - Single LSTM hidden layer to extract features from the sequence\n",
    "    - Followed by a fully connected layer to interpret the LSTM output\n",
    "    - Followed by output layer for making binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1af894491edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvisible\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhidden1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mhidden2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;31m# LSTM does not support constants. Ignore it during process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       init_state = get_initial_state_fn(\n\u001b[0m\u001b[0;32m    646\u001b[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[0;32m    647\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m     return list(_generate_zero_filled_state_for_cell(\n\u001b[0m\u001b[0;32m   2524\u001b[0m         self, inputs, batch_size, dtype))\n\u001b[0;32m   2525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[1;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2966\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2967\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2968\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[1;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[0;32m   2982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2984\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2985\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2986\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[1;34m(unnested_state_size)\u001b[0m\n\u001b[0;32m   2979\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2981\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2746\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2747\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2748\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   2792\u001b[0m           \u001b[1;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2793\u001b[0m           \u001b[1;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2794\u001b[1;33m           \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2795\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2796\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[1;34m(value, shape, dtype, name)\u001b[0m\n\u001b[0;32m   2730\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2731\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2732\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2733\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2734\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   3028\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m     \"\"\"\n\u001b[1;32m-> 3030\u001b[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[0;32m   3031\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   3032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vnwk_dl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m     raise NotImplementedError(\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[1;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[1;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "# example of a recurrent neural network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "visible = Input(shape=(100,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()\n",
    "\n",
    "# plot graph\n",
    "plot_model(model, to_file='recurrent_neural_network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Image Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load and handle image data using PIL/ Pillow standard Python library\n",
    "* Manually standardize and normalize pixel values to prepare images for modeling\n",
    "* Load and handle image data directly using Keras DL library\n",
    "* Scale pixel values to prepare images for modeling using Keras\n",
    "* Load large image datasets from directories using Keras\n",
    "* Use data augmentation to expand the size of an image training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow Version: 8.2.0\n"
     ]
    }
   ],
   "source": [
    "# check Pillow version number\n",
    "import PIL\n",
    "print('Pillow Version:', PIL.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to load and manipulate images with Keras**\n",
    "* Test Image\n",
    "* Keras Image Processing API\n",
    "* Load an Image with Keras\n",
    "* Convert an Image with Keras\n",
    "* Save an Image with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading image from file as a PIL iamge object\n",
    "    - load_img() function\n",
    "* Convert loaded image in PIL format into a NumPy array for use with DL methods\n",
    "    - img_to_array() function\n",
    "    - arary_to_img() function to convert NumPy array of pixel data into a PIL image\n",
    "* Save image to file\n",
    "    - save_img() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "None\n",
      "RGB\n",
      "(640, 427)\n"
     ]
    }
   ],
   "source": [
    "# example of saving an image with the Keras API\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# load image as as grayscale\n",
    "img = load_img('bondi_beach.jpg', color_mode='grayscale')\n",
    "\n",
    "# convert image to a numpy array\n",
    "img_array = img_to_array(img)\n",
    "\n",
    "# save the image with a new filename\n",
    "save_img('bondi_beach_grayscale.jpg', img_array)\n",
    "\n",
    "# load the image to confirm it was saved correctly\n",
    "img = load_img('bondi_beach_grayscale.jpg')\n",
    "print(type(img))\n",
    "print(img.format)\n",
    "print(img.mode)\n",
    "print(img.size)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
